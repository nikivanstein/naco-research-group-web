---
layout: post
shortnews: false
icon: newspaper-o
title:  "Natural Computing at GECCO 2025: 10 Papers Accepted"
image: /img/posts/naco@gecco.png
header:
    og_image: posts/naco@gecco.png
---

**Leiden, The Netherlands** – The Natural Computing research cluster at LIACS, Leiden University is proud to share that ten of our papers — plus one *Hot Off the Press* and one *Competition* paper — have been accepted for presentation at the [Genetic and Evolutionary Computation Conference (GECCO) 2025](https://gecco-2025.sigevo.org/HomePage).

GECCO is the premier venue in the field of evolutionary computation, drawing international researchers working at the forefront of algorithm design, optimization, and learning. This year’s accepted contributions highlight the breadth of our ongoing work — from LLM-assisted algorithm discovery and quantum-inspired optimization, to robust strategies for noisy and diverse search spaces.

---

### Our Accepted Contributions

Here's an overview of our accepted papers:


#### Code Evolution Graphs: Understanding Large Language Model Driven Design of Algorithms

**Authors**: _Niki van Stein, Anna V. Kononova, Lars Kotthoff, Thomas Bäck_
**Abstract**: Large Language Models (LLMs) have demonstrated great promise in generating code, especially when used inside an evolutionary computation framework to iteratively optimize the generated algorithms. However, in some cases they fail to generate competitive algorithms or the code optimization stalls, and we are left with no recourse because of a lack of understanding of the generation process and generated codes. We present a novel approach to mitigate this problem by enabling users to analyze the generated codes inside the evolutionary process and how they evolve over repeated prompting of the LLM. We show results for three benchmark problem classes and demonstrate novel insights. In particular, LLMs tend to generate more complex code with repeated prompting, but additional complexity can hurt algorithmic performance in some cases. Different LLMs have different coding ``styles'' and generated code tends to be dissimilar to other LLMs. These two findings suggest that using different LLMs inside the code evolution frameworks might produce higher performing code than using only one LLM.

#### Cascading CMA-ES Instances for Generating Input-diverse Solution Batches

**Authors**: _Maria Laura Santoni, Christoph Dürr, Carola Doerr, Mike Preuss, and Elena Raponi_
**Abstract**: Rather than obtaining a single good solution for a given optimization problem, users often seek alternative design choices, because the best-found solution may perform poorly with respect to additional objectives or constraints that are difficult to capture into the modeling process.
Aiming for batches of diverse solutions of high quality is often desirable, as it provides flexibility to accommodate post-hoc user preferences. At the same time, it is crucial that the quality of the best solution found is not compromised.
One particular problem setting balancing high quality and diversity is fixing the required minimum distance between solutions while simultaneously obtaining the best possible fitness.
Recent work by Santoni et al. [arXiv 2024] revealed that this setting is not well addressed by state-of-the-art algorithms, performing in par or worse than pure random sampling.
Driven by this important limitation, we propose a new approach, where parallel runs of the covariance matrix adaptation evolution strategy (CMA-ES) inherit tabu regions in a cascading fashion.
We empirically demonstrate that our CMA-ES-Diversity Search (CMA-ES-DS) algorithm generates trajectories that allow to extract high-quality solution batches that respect a given minimum distance requirement, clearly outperforming those obtained from off-the-shelf random sampling, multi-modal optimization algorithms, and standard CMA-ES.
See [https://arxiv.org/abs/2502.13730](https://arxiv.org/abs/2502.13730) for the Arxiv version.


#### Optimizing Photonic Structures with Large Language Model Driven Algorithm Discovery

**Authors**: _Haoran Yin, Anna V. Kononova, Thomas Bäck, Niki van Stein_
**Abstract**: We study how large language models can be used in combination with evolutionary computation techniques to automatically discover optimization algorithms for the design of photonic structures. Building on the Large Language Model Evolutionary Algorithm (LLaMEA) framework, we introduce structured prompt engineering tailored to multilayer photonic problems such as Bragg mirror, ellipsometry inverse analysis, and solar cell antireflection coatings. We systematically explore multiple evolutionary strategies, including (1+1), (1+5), (2+10), and others, to balance exploration and exploitation. Our experiments show that LLM-generated algorithms, generated using small-scale problem instances, can match or surpass established methods like quasi-oppositional differential evolution on large-scale realistic real-world problem instances. Notably, LLaMEA's self-debugging mutation loop, augmented by automatically extracted problem-specific insights, achieves strong anytime performance and reliable convergence across diverse problem scales. This work demonstrates the feasibility of domain-focused LLM prompts and evolutionary approaches in solving optical design tasks, paving the way for rapid, automated photonic inverse design.


#### BLADE: Benchmark suite for LLM-driven Automated Design and Evolution of iterative optimisation heuristics

**Authors**: _Niki van Stein, Anna V. Kononova, Haoran Yin, Thomas Bäck_
**Abstract**: The application of Large Language Models (LLMs) for Automated Algorithm Discovery (AAD), particularly for optimisation heuristics, is an emerging field of research. This emergence necessitates robust, standardised benchmarking practices to rigorously evaluate the capabilities and limitations of LLM-driven AAD methods and the resulting generated algorithms, especially given the opacity of their design process and known issues with existing benchmarks. To address this need, we introduce BLADE (Benchmark suite for LLM-driven Automated Design and Evolution), a modular and extensible framework specifically designed for benchmarking LLM-driven AAD methods in a continuous black-box optimisation context. BLADE integrates collections of benchmark problems (including MA-BBOB and SBOX-COST among others) with instance generators and textual descriptions aimed at capability-focused testing, such as generalisation, specialisation and information exploitation. It offers flexible experimental setup options, standardised logging for reproducibility and fair comparison, incorporates methods for analysing the AAD process (e.g., Code Evolution Graphs and various visualisation approaches) and facilitates comparison against human-designed baselines through integration with established tools like IOHanalyser and IOHexplainer. BLADE provides an `out-of-the-box' solution to systematically evaluate LLM-driven AAD approaches. The framework is demonstrated through two distinct use cases exploring mutation prompt strategies and function specialisation.


### Hot Off the Press papers

1. LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics
2. Explainable Benchmarking for Iterative Optimization Heuristics

### Competition papers

1. Neighborhood Adaptive Differential Evolution

---

### Wrapping Up

These papers reflect our ongoing focus on foundational methods, real-world relevance, and emerging trends like LLMs, surrogate modeling, and quantum-inspired optimization. We're excited to share and discuss these ideas with the GECCO community in July.

Stay tuned for more updates and deeper dives as we approach the conference.  
Follow us on <a href="https://linkedin.com/company/naco-liacs/" target="_blank">LinkedIn</a> for news, preprints, and post-conference impressions.
