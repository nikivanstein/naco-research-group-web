---
title: "LLMfwEC Workshop at GECCO 2025"
status: active
type: workshop

notitle: false

description: |
  Large Language Models for and with Evolutionary Computation


eventdate: 2025-07-14
layout: event
image: /img/events/llmfwec-header.png
header:
    og_image: events/llmfwec.png
last-updated: 2025-01-27
---

We are excited to announce the **LLMfwEC Workshop at GECCO 2025**. This workshop explores the convergence of **Large Language Models (LLMs)** and **Evolutionary Computing (EC)**, aiming to advance research through novel methodologies and applications in both fields.


<a class="btn btn-success" href="https://ssl.linklings.net/conferences/gecco/" target="_blank">Submit your paper</a>

## Large Language Models for and with Evolutionary Computation

Large language models (LLMs), along with other foundational models in generative AI, have significantly changed the traditional expectations of artificial intelligence and machine learning systems. An LLM takes natural language text prompts as input and generates responses by matching patterns and completing sequences, providing output in natural language. In contrast, evolutionary computation (EC) is inspired by Neo-Darwinian evolution and focuses on black-box search and optimization. But what connects these two approaches?

One answer is evolutionary search heuristics (LLM with EC), with operators that use LLMs to fulfill their function. This hybridization turns the conventional paradigm that ECs use on its head, and in turn, sometimes yields high-performing and novel EC systems.

Another answer is using LLM for EC. LLMs may help researchers select feasible candidates from the pool of algorithms based on user-specified goals and provide a basic description of the methods or propose novel hybrid methods. Further, the models can help identify and describe distinct components suitable for adaptive enhancement or hybridization and provide a pseudo-code, implementation, and reasoning for the proposed methodology. Finally, LLMs have the potential to transform automated metaheuristic design and configuration by generating codes, iteratively improving the initially designed solutions or algorithm templates (with or without performance or other data-driven feedback), and even guiding implementations.

This workshop aims to encourage innovative approaches that leverage the strengths of LLMs and EC techniques, thus enabling the creation of more adaptive, efficient, and scalable algorithms by integrating evolutionary mechanisms with advanced LLM capabilities. Thanks to the collaborative platform for researchers and practitioners, the workshop may Inspire novel research directions that could reshape AI, specifically LLMs, and optimization fields through this hybridization and achieve a better understanding and explanation of how these two seemingly disparate fields are related and how knowledge of their functions and operations can be leveraged.


### Topics of Interest

The intersection of LLMs and Evolutionary Computing brings forth various topics of interest, including but not limited to:

- Evolutionary Prompt Engineering
- Optimisation of LLM Architectures
- LLM-Guided Evolutionary Algorithms
- How can an EA using an LLM evolve different of units of evolution, e.g. code, strings, images, multi-modal candidates?
- How can an EA using an LLM solve prompt composition or other LLM development and use challenges?
- How can an EA using an LLM integrate design explorations related to cooperation, modularity, reuse, or competition?
- How can an EA using an LLM model biology?
- How can an EA using an LLM intrinsically, or with guidance, support open-ended evolution?
- What new variants hybridizing EC and/or another search heuristic are possible and in what respects are they advantageous?
- What are new ways of using LLMs for evolutionary operators, e.g. new ways of generating variation through LLMs, as with LMX or ELM, or new ways of using LLMs for selection, as with e.g. Quality-Diversity through AI Feedback
- How well does an EA using an LLM scale with population size and problem complexity?
- What is the most accurate computational complexity of an EA using an LLM?
- What makes good EA plus LLM benchmark?
- LLMs for (automated) generation of EC.
- Understanding, fine-tuning, and adaptation of Large Language Models for EC. How large do LLMs need to be? Are there benefits for using larger/smaller ones? Ones trained on different datasets or in different ways?
- Implementing/generating methodology for population dynamics analysis, population diversity measures, control, and analysis and visualization.
- Generating rules for EC (boundary and constraints handling strategies).
- The performance improvement, testing, and efficiency of the improved algorithms.
- Reasoning for component-wise analysis of algorithms.
- Connection of LLM and other ML techniques for EC (Reinforcement learning, AutoML)
- Generation and reasoning for parallel approaches for EC algorithms.
- Benchmarking and Comparative Studies of LLM-generated algorithms.
- Applications of LLM and EC (not limited to):
- constrained optimization
- multi-objective optimization
- expensive and surrogate assisted optimization
- dynamic and uncertain optimization
- large-scale optimization
- combinatorial/discrete optimization

### Why Attend?

- **Explore Emerging Research**: Engage with pioneering techniques combining LLMs and EC to push AI's boundaries.
- **Networking**: Connect with experts in both fields and form impactful collaborations.
- **Practical Insights**: Gain insights into how LLM and EC integration can benefit real-world applications.

### Key Dates

- Submission opening: February 10, 2025
- Submission deadline: April 2 (Extended!), 2025
- Notification: April 28, 2025
- Camera-ready: May 5, 2025
- Author's mandatory registration: May 8, 2025

<a class="btn btn-success" href="https://ssl.linklings.net/conferences/gecco/" target="_blank">Submit your paper</a>

Join us and be part of the community shaping the future of AI through LLM-EC hybridization!

### Previous Editions

- (GECCO 2024)[https://sites.google.com/view/llmfwec-2024]

### Organizers

**Kevin Ellis** is a professor of Computer Science at Cornell University. His research agenda includes the topic of AI and program synthesis while he also studies AI’s interactions with Cognitive Science. His DreamMaker system is highly recognised and he bridges the ML and program synthesis community and the evolutionary computation community. See https://www.cs.cornell.edu/~ellisk/  

**Erik Hemberg** is a Research Scientist in the AnyScale Learning For All (ALFA) group at Massachusetts Institute of Technology Computer Science and Artificial Intelligence Lab, USA. He has a PhD in Computer Science from University College Dublin, Ireland and an MSc in Industrial Engineering and Applied Mathematics from Chalmers University of Technology, Sweden. His research interests involves coevolutionary algorithms, grammatical representations and generative models. His work focuses on developing autonomous, pro-active cyber defenses that are anticipatory and adapt to counter attacks. He is also interested in automated semantic parsing of law, and data science for education and healthcare.

**Roman Senkerik** works as a professor and head of the A.I.Lab ( https://ailab.fai.utb.cz/ ) at the Department of Informatics and Artificial Intelligence at Tomas Bata University in Zlin. He is also employed as a professor at the Department of Computer Science, Faculty of Electrical Engineering and Computer Science, VŠB - Technical University Ostrava. He obtained his master’s degree in Technical Cybernetics at the Tomas Bata University in Zlin, Faculty of Applied Informatics, in 2004, and his PhD degree in Technical Cybernetics in 2008 at the same university. In 2013, he habilitated an associate Professor) in Informatics, and in the same field, he obtained the title of Full Professor at VŠB - Technical University Ostrava in 2022. His primary research interests include generative artificial intelligence, bio-inspired optimization methods, and distributed systems. Specifically, his work focuses on the development of evolutionary algorithms, machine learning models and their modifications, benchmarking, generative AI (LLMs), AutoML, neuroevolution, data science, and interdisciplinary applications of soft computing and machine learning methods. He has been and is part of many teams for special sessions/symposia and event organizations related to AI and metaheuristics at conferences such as IEEE WCCI, IEEE CEC, IEEE SSCI or GECCO. He has been guest editor for several special issues of journals and edited the proceedings of several conferences. He is a Fellow of the IEEE, a member of the IEEE CIS and IEEE SMC groups, and a member of the IEEE task force on benchmarking metaheuristics.

**Joel Lehman** is a machine learning researcher interested in algorithmic creativity, evolutionary algorithms, artificial life, and AI for wellbeing. Most recently he was a research scientist at OpenAI co-leading the Open-Endedness team (studying algorithms that can innovate endlessly). Previously he was a founding member of Uber AI Labs, first employee of Geometric Intelligence (acquired by Uber), and a tenure track professor at the IT University of Copenhagen. He co-wrote with Kenneth Stanley a popular science book called "Why Greatness Cannot Be Planned" on what AI search algorithms imply for individual and societal accomplishment.

**Una-May O'Reilly** is the leader of ALFA Group at Massachusetts Institute of Technology's Computer Science and Artificial Intelligence Lab. An evolutionary computation researcher for 20+ years, she is broadly interested in adversarial intelligence — the intelligence that emerges and is recruited while learning and adapting in competitive settings. Her interest has led her to study settings where security is under threat, for which she has developed machine learning algorithms that variously model the arms races of tax compliance and auditing, malware and its detection, cyber network attacks and defenses, and adversarial paradigms in deep learning. She is passionately interested in programming and genetic programming. She is a recipient of the EvoStar Award for Outstanding Achievements in Evolutionary Computation in Europe and the ACM SIGEVO Award Recognizing Outstanding Achievements in Evolutionary Computation. Devoted to the field and committed to its growth, she served on the ACM SIGEVO executive board from SIGEVO's inception and held different officer positions before retiring from it in 2023. She co-founded the annual workshops for Women@GECCO and has proudly watched their evolution to Women+@GECCO. She was on the founding editorial boards and continues to serve on the editorial boards of Genetic Programming and Evolvable Machines, and ACM Transactions on Evolutionary Learning and Optimization. She has received a GECCO best paper award and an GECCO test of time award. She is honored to be a member of SPECIES, a member of the Julian Miller Award committee, and to chair the 2023 and 2024 committees selecting SIGEVO Awards Recognizing Outstanding Achievements in Evolutionary Computation.

**Michal Pluhacek** is the ARTIQ project leader and professor at the AGH University of Krakow. His research interests include diverse branches of artificial intelligence, e.g. evolutionary computation, swarm intelligence, and, more recently, the applications of large language models. He has extensive international experience and numerous publications at world-leading congresses, conferences, and in respected journals.
Prof. Pluhacek received his Ph.D. degree in Information Technologies in 2016. His dissertation topic was “Modern Method of Development and Modifications of Evolutionary Computational Techniques.” Later, he was awarded the permanent assoc. prof. title in 2023 after successfully defending his habilitation thesis on the topic “Inner Dynamics of Evolutionary Computation Techniques: Meaning for Practice.“

**Niki van Stein** is an assistant professor at Leiden University, leading the XAI research group within the Natural Computing Cluster. Her work bridges the fields of evolutionary computation and explainable AI, with recent research advancing the integration of large language models (LLMs) into evolutionary frameworks, including the development of methods like LLaMEA and LLaMEA-HPO. These innovations address challenges in utilizing LLMs automatic algorithm discovery and optimization for black-box problems.  Dr. van Stein received her Ph.D. degree in Computer Science in 2018. Her expertise is recognized internationally through publications and presentations at leading conferences, and she is known for her contributions to practical applications in predictive maintenance, design optimization, and decision support.

**Pier Luca Lanzi** received the Laurea degree in computer science from the Université degli Studi di Udine and the Ph.D. degree in Computer and Automation Engineering from the Politecnico di Milano. He is an associate professor at the Politecnico di Milano, Dept. of Electronics and Information. His research areas include genetic and evolutionary computation, reinforcement learning, and machine learning. He is interested in applications to data mining and autonomous agents. He is member of the editorial board of the "Evolutionary Computation Journal" and Editor in chief of SIGEVOlution, the ACM Newsletter of SIGEVO, the Special Interest Group on Genetic and Evolutionary Computation.

**Tome Eftimov** is a researcher at the Computer Systems Department at the Jožef Stefan Institute, Ljubljana, Slovenia. He is a visiting assistant professor at the Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, Skopje. He was a postdoctoral research fellow at the Stanford University, USA, where he investigated biomedical relations outcomes by using AI methods. In addition, he was a research associate at the University of California, San Francisco, investigating AI methods for rheumatology concepts extraction from electronic health records. He obtained his PhD in Information and Communication Technologies (2018). His research interests include statistical data analysis, metaheuristics, natural language processing, representation learning, and machine learning. He has been involved in courses on probability and statistics, and statistical data analysis. The work related to Deep Statistical Comparison was presented as tutorial (i.e. IJCCI 2018, IEEE SSCI 2019, GECCO 2020, and PPSN 2020) or as invited lecture to several international conferences and universities. He is an organizer of several workshops related to AI at high-ranked international conferences. He is a coordinator of a national project “Mr-BEC: Modern approaches for benchmarking in evolutionary computation” and actively participates in European projects. 
